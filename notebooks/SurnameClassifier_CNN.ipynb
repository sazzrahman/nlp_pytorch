{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the same Surname classifier using CNN\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from argparse import Namespace\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Version descrepancies on this one\n",
    "from tqdm.notebook import tqdm as notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../cnn_helper/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from cnn_classifier import SurnameClassifier\n",
    "from cnn_utils import ModelUtils\n",
    "from cnn_dataset import SurnameDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "# Data and path info\n",
    "surname_csv = \"../data/surnames_with_splits.csv\",\n",
    "vectorizer_file = \"vectorizer.json\",\n",
    "model_state_file = \"model.pth\",\n",
    "save_dir = \"../data/model_state\",\n",
    "# Model Hyperparameters\n",
    "hidden_dim = 100,\n",
    "num_channels = 256,\n",
    "# Training Hyper Parameters\n",
    "seed = 1337,\n",
    "batch_size = 128,\n",
    "num_epochs = 100,\n",
    "early_stopping_criteria=5,\n",
    "learning_rate = 0.001,\n",
    "dropout_p=0.1,\n",
    "# Runtime operations\n",
    "cuda = False,\n",
    "reload_from_files=False,\n",
    "expand_filepaths_to_save_dir=True,\n",
    "catch_keyboard_interrupt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\t../data/model_state\\vectorizer.json\n",
      "\t../data/model_state\\model.pth\n"
     ]
    }
   ],
   "source": [
    "# Expands filepath if none available\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA\n",
    "if torch.cuda.is_available():\n",
    "    args.cuda = True\n",
    "    \n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelUtils.set_seed_everywhere(args.seed,args.cuda)\n",
    "ModelUtils.handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating fresh!\n"
     ]
    }
   ],
   "source": [
    "# Model Initializations\n",
    "if args.reload_from_files:\n",
    "    print(\"Reloading !\")\n",
    "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,\n",
    "                                                              args.vectorizer_file)\n",
    "    \n",
    "else:\n",
    "    print(\"Creating fresh!\")\n",
    "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "    \n",
    "vectorizer = dataset.get_vectorizer()\n",
    "classifier = SurnameClassifier(initial_num_channels = len(vectorizer.surname_vocab),\n",
    "                              num_classes = len(vectorizer.nationality_vocab),\n",
    "                              num_channels = args.num_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "\n",
    "# Whyn am I feeding class weights to loss function ??\n",
    "loss_func = nn.CrossEntropyLoss(weight = dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(),lr = args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min',factor=0.5,patience=1)\n",
    "\n",
    "train_state = ModelUtils.make_train_state(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073b2bbd845f4d56938b16feb43cd848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training routine', style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665c59f400f04cda9fc8e3da78483a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='split=train', max=60.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41951838e1f8404dbc18be309b865f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='split=validation', max=12.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting Loop\n"
     ]
    }
   ],
   "source": [
    "epoch_bar = notebook(desc = 'training routine',\n",
    "                         total = args.num_epochs,\n",
    "                         position = 0\n",
    "                         )\n",
    "dataset.set_split('train')\n",
    "train_bar = notebook(desc = 'split=train',\n",
    "                         total = dataset.get_num_batches(args.batch_size),\n",
    "                         position=1,\n",
    "                         leave=True)\n",
    "\n",
    "dataset.set_split('validation')\n",
    "val_bar = notebook(desc = 'split=validation',\n",
    "                         total = dataset.get_num_batches(args.batch_size),\n",
    "                         position=1,\n",
    "                         leave=True)\n",
    "\n",
    "\n",
    "try:    \n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        # for the first iteration train_state gets default values\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "        \n",
    "        # dataset mode : training\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = ModelUtils.generate_batches(dataset,batch_size=args.batch_size,device=args.device)\n",
    "        \n",
    "        #running loss and acc will reset to zero\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # predicting value just by passing value to classifier. \n",
    "            y_pred = classifier(batch_dict['x_surname'])\n",
    "            loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
    "            loss_t = loss.item()\n",
    "            \n",
    "            ## why running loss needs to be divided by batch index?\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # compute running accuracy\n",
    "            acc_t = ModelUtils.compute_accuracy(y_pred,batch_dict['y_nationality'])\n",
    "            \n",
    "            # why divide by batch index?\n",
    "            running_acc += (acc_t - running_acc)/(batch_index + 1)\n",
    "            \n",
    "            # update the running loss and running accuracy for training bar. \n",
    "            train_bar.set_postfix(loss=running_loss,acc=running_acc, epoch = epoch_index)\n",
    "            train_bar.update()\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "        \n",
    "        # dataset mode validation\n",
    "        # cleaning up before validation routine\n",
    "        batch_generator = ModelUtils.generate_batches(dataset,batch_size=args.batch_size,device=args.device)\n",
    "        running_loss = 0\n",
    "        running_acc = 0\n",
    "        classifier.eval()\n",
    "        dataset.set_split(\"validation\")\n",
    "        \n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator): \n",
    "            # predicting value just by passing value to classifier. \n",
    "            y_pred = classifier(batch_dict['x_surname'])\n",
    "            loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "            \n",
    "            acc_t = ModelUtils.compute_accuracy(y_pred,batch_dict['y_nationality'])\n",
    "            running_acc += (acc_t - running_acc)/(batch_index + 1)\n",
    "            \n",
    "            val_bar.set_postfix(loss=running_loss,acc=running_acc,epoch = epoch_index)\n",
    "            val_bar.update()\n",
    "            \n",
    "            \n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "        \n",
    "        \n",
    "        train_state = ModelUtils.update_train_state(args=args,model=classifier,train_state=train_state)\n",
    "        \n",
    "        # has to do with learning rate adjustment\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "        \n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting Loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Must use map_location parameter for deserializing to cpu.\n",
    "# Load pretrained weights to classifier.\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename'],map_location=torch.device(args.device) ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = ModelUtils.generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # this is the pretrained classifier aka the model.\n",
    "    y_pred =  classifier(batch_dict['x_surname'])\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = ModelUtils.compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.780891795953115, 54.296875)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_state['test_loss'], train_state['test_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All tensors must go to device\n",
    "# Where apply softmax technique is finally used\n",
    "def predict_nationality(surname, classifier, vectorizer, device=args.device):\n",
    "    \"\"\"Predict the nationality from a new surname\n",
    "    \n",
    "    Args:\n",
    "        surname (str): the surname to classifier\n",
    "        classifier (SurnameClassifer): an instance of the classifier\n",
    "        vectorizer (SurnameVectorizer): the corresponding vectorizer\n",
    "    Returns:\n",
    "        a dictionary with the most likely nationality and its probability\n",
    "    \"\"\"\n",
    "    vectorized_surname = vectorizer.vectorize(surname)\n",
    "    vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(0)\n",
    "    # must send the new tensor to device\n",
    "    vectorized_surname = vectorized_surname.to(device)\n",
    "    result = classifier(vectorized_surname, apply_softmax=True)\n",
    "\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    index = indices.item()\n",
    "\n",
    "    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "    probability_value = probability_values.item()\n",
    "\n",
    "    return {'nationality': predicted_nationality, 'probability': probability_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_surname = input(\"Enter a surname to classify: \")\n",
    "classifier = classifier.to(args.device)\n",
    "prediction = predict_nationality(new_surname, classifier, vectorizer)\n",
    "print(\"{} -> {} (p={:0.2f})\".format(new_surname,\n",
    "                                    prediction['nationality'],\n",
    "                                    prediction['probability']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topk_nationality(name, classifier, vectorizer, k=5, device=args.device):\n",
    "    vectorized_name = vectorizer.vectorize(name)\n",
    "    vectorized_name = torch.tensor(vectorized_name).unsqueeze(dim=0)\n",
    "    vectorized_name = vectorized_name.to(device)\n",
    "    prediction_vector = classifier(vectorized_name, apply_softmax=True)\n",
    "    probability_values, indices = torch.topk(prediction_vector, k=k)\n",
    "    \n",
    "    # returned size is 1,k\n",
    "    # cpu conversion is necessary because tensor object cannot directly interact with numpy\n",
    "    probability_values = probability_values.detach().cpu().numpy()[0]\n",
    "    indices = indices.detach().cpu().numpy()[0]\n",
    "    \n",
    "    results = []\n",
    "    for prob_value, index in zip(probability_values, indices):\n",
    "        nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "        results.append({'nationality': nationality, \n",
    "                        'probability': prob_value})\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_surname = input(\"Enter a surname to classify: \")\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "k = int(input(\"How many of the top predictions to see? \"))\n",
    "if k > len(vectorizer.nationality_vocab):\n",
    "    print(\"Sorry! That's more than the # of nationalities we have.. defaulting you to max size :)\")\n",
    "    k = len(vectorizer.nationality_vocab)\n",
    "    \n",
    "predictions = predict_topk_nationality(new_surname, classifier, vectorizer, k=k)\n",
    "\n",
    "print(\"Top {} predictions:\".format(k))\n",
    "print(\"===================\")\n",
    "for prediction in predictions:\n",
    "    print(\"{} -> {} (p={:0.2f})\".format(new_surname,\n",
    "                                        prediction['nationality'],\n",
    "                                        prediction['probability']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
